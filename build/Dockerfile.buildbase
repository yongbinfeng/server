
ARG TRITON_VERSION=2.53.0dev
ARG TRITON_CONTAINER_VERSION=24.12dev
ARG BASE_IMAGE=almalinux:8.9

FROM ${BASE_IMAGE}

ARG TRITON_VERSION
ARG TRITON_CONTAINER_VERSION
ENV PIP_BREAK_SYSTEM_PACKAGES=1

# Install docker docker buildx
RUN yum install -y ca-certificates curl gnupg yum-utils \
      && yum-config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo \
      && yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
#   && yum install -y docker.io docker-buildx-plugin

# libcurl4-openSSL-dev is needed for GCS
# python3-dev is needed by Torchvision
# python3-pip and libarchive-dev is needed by python backend
# libxml2-dev is needed for Azure Storage
# scons is needed for armnn_tflite backend build dep
RUN yum install -y epel-release
RUN yum install --enablerepo=powertools -y \
            ca-certificates \
            autoconf \
            automake \
            gcc-toolset-9 \
            gcc-toolset-9-libstdc++-devel \
            cmake \
            git \
            gperf \
            re2-devel \
            libffi-devel \
            openssl-devel \
            libtool \
            libcurl-devel \
            libb64-devel \
            gperftools-devel \
            patchelf \
            python3-pip \
            python3-setuptools \
            rapidjson-devel \
            python3-scons \
            pkg-config \
            unzip \
            wget \
            ncurses-devel \
            readline-devel \
            xz-devel \
            bzip2-devel \
            zlib-devel \
            libarchive-devel \
            libxml2-devel \
            numactl-devel \
            wget

RUN scl enable gcc-toolset-9 bash

# The python library version available for install via 'yum install python3.X-devel' does not
# match the version of python inside the RHEL base container. This means that python packages
# installed within the container will not be picked up by the python backend stub process pybind
# bindings. It must instead must be installed via pyenv.
ENV PYENV_ROOT=/opt/pyenv_build
RUN curl https://pyenv.run | bash
ENV PATH="${PYENV_ROOT}/bin:$PATH"
RUN eval "$(pyenv init -)"
RUN CONFIGURE_OPTS="--with-openssl=/usr/lib64" && pyenv install 3.12.3 \
    && cp ${PYENV_ROOT}/versions/3.12.3/lib/libpython3* /usr/lib64/

# RHEL image has several python versions. It's important
# to set the correct version, otherwise, packages that are
# pip installed will not be found during testing.
ENV PYVER=3.12.3 PYTHONPATH=/opt/python
RUN ln -sf ${PYENV_ROOT}/versions/${PYVER}* ${PYTHONPATH}
RUN ln -sf ${PYTHONPATH}/bin/python3 /usr/bin/python3
ENV PYBIN=${PYTHONPATH}/bin
ENV PYTHON_BIN_PATH=${PYBIN}/python${PYVER} PATH=${PYBIN}:${PATH}


RUN pip3 install --upgrade pip \
      && pip3 install --upgrade \
          build \
          wheel \
          setuptools \
          docker \
          virtualenv

# Install boost version >= 1.78 for boost::span
# Current libboost-dev apt packages are < 1.78, so install from tar.gz
RUN wget -O /tmp/boost.tar.gz \
          https://archives.boost.io/release/1.80.0/source/boost_1_80_0.tar.gz \
      && (cd /tmp && tar xzf boost.tar.gz) \
      && mv /tmp/boost_1_80_0/boost /usr/include/boost

# Server build requires recent version of CMake (FetchContent required)
# Might not need this if the installed version of cmake is high enough for our build.
# RUN apt update -q=2 \
#       && apt install -y gpg wget \
#       && wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - |  tee /usr/share/keyrings/kitware-archive-keyring.gpg >/dev/null \
#       && . /etc/os-release \
#       && echo "deb [signed-by=/usr/share/keyrings/kitware-archive-keyring.gpg] https://apt.kitware.com/ubuntu/ $UBUNTU_CODENAME main" | tee /etc/apt/sources.list.d/kitware.list >/dev/null \
#       && apt-get update -q=2 \
#       && apt-get install -y --no-install-recommends cmake=3.27.7* cmake-data=3.27.7*


# Install CUDA Toolkit
RUN dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo \
    && dnf clean expire-cache \
    && dnf install -y cuda-toolkit-12-6

RUN dnf -y module install nvidia-driver:latest-dkms

RUN dnf -y install cudnn9-cuda-12

ENV DCGM_VERSION 3.3.6
# Install DCGM. Steps from https://developer.nvidia.com/dcgm#Downloads
RUN dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo \
    && dnf clean expire-cache \
    && dnf install -y datacenter-gpu-manager-3.3.6

ENV TRITON_SERVER_VERSION ${TRITON_VERSION}
ENV NVIDIA_TRITON_SERVER_VERSION ${TRITON_CONTAINER_VERSION}

ENV CUDACXX /usr/local/cuda-12/bin/nvcc
ENV CUDNN_VERSION 9.6.0

ENV PATH="/opt/rh/gcc-toolset-9/root/usr/bin:$PATH"
ENV LD_LIBRARY_PATH="/opt/rh/gcc-toolset-9/root/usr/lib64:$LD_LIBRARY_PATH"

RUN echo "source scl_source enable gcc-toolset-9" >> ~/.bashrc

WORKDIR /workspace
RUN rm -fr *
COPY . .
ENTRYPOINT []
